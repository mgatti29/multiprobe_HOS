{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba4d4354-c5c0-4e7b-a9f0-ed57d3b271d5",
   "metadata": {},
   "source": [
    "# Make shear maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd597463-2715-44e7-8ec4-c6fe3846f0d3",
   "metadata": {},
   "source": [
    "This notebook generates DES-like shear maps out of particle count maps from N-body sims."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620dac54-c685-4c52-a103-4f02f5812cd2",
   "metadata": {},
   "source": [
    "# routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fdab28d-7d00-4d06-9eeb-c6edd76fd32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import healpy as hp\n",
    "import asdf\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "from bornraytrace import lensing as brk\n",
    "from bornraytrace import intrinsic_alignments as iaa\n",
    "import bornraytrace\n",
    "from astropy.cosmology import FlatLambdaCDM,wCDM\n",
    "from astropy import units as u\n",
    "import os\n",
    "from astropy.table import Table  \n",
    "from astropy.cosmology import z_at_value\n",
    "import astropy.io.fits as fits\n",
    "import pickle\n",
    "\n",
    "def save_obj(name, obj):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, protocol=2)\n",
    "        f.close()\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        mute =  pickle.load(f)\n",
    "        f.close()\n",
    "    return mute\n",
    "\n",
    "def rotate_map_approx(mask, rot_angles, flip=False,nside = 1024):\n",
    "    alpha, delta = hp.pix2ang(nside, np.arange(len(mask)))\n",
    "\n",
    "    rot = hp.rotator.Rotator(rot=rot_angles, deg=True)\n",
    "    rot_alpha, rot_delta = rot(alpha, delta)\n",
    "    if not flip:\n",
    "        rot_i = hp.ang2pix(nside, rot_alpha, rot_delta)\n",
    "    else:\n",
    "        rot_i = hp.ang2pix(nside, np.pi-rot_alpha, rot_delta)\n",
    "    rot_map = mask*0.\n",
    "    rot_map[rot_i] =  mask[np.arange(len(mask))]\n",
    "    return rot_map\n",
    "\n",
    "def gk_inv(K,KB,nside,lmax):\n",
    "\n",
    "    alms = hp.map2alm(K, lmax=lmax, pol=False)  # Spin transform!\n",
    "\n",
    "    ell, emm = hp.Alm.getlm(lmax=lmax)\n",
    "\n",
    "    kalmsE = alms/( 1. * ((ell * (ell + 1.)) / ((ell + 2.) * (ell - 1))) ** 0.5)\n",
    "   \n",
    "    kalmsE[ell == 0] = 0.0\n",
    "\n",
    "    \n",
    "    alms = hp.map2alm(KB, lmax=lmax, pol=False)  # Spin transform!\n",
    "\n",
    "    ell, emm = hp.Alm.getlm(lmax=lmax)\n",
    "\n",
    "    kalmsB = alms/( 1. * ((ell * (ell + 1.)) / ((ell + 2.) * (ell - 1))) ** 0.5)\n",
    "   \n",
    "    kalmsB[ell == 0] = 0.0\n",
    "\n",
    "    _,e1t,e2t = hp.alm2map([kalmsE,kalmsE,kalmsB] , nside=nside, lmax=lmax, pol=True)\n",
    "    return e1t,e2t# ,r\n",
    "\n",
    "\n",
    "\n",
    "def random_draw_ell_from_w(wi,w,e1,e2):\n",
    "    '''\n",
    "    wi: input weights\n",
    "    w,e1,e2: all the weights and galaxy ellipticities of the catalog.\n",
    "    e1_,e2_: output ellipticities drawn from w,e1,e2.\n",
    "    '''\n",
    "\n",
    "\n",
    "    ell_cont = dict()\n",
    "    for w_ in np.unique(w):\n",
    "        mask_ = w == w_\n",
    "        w__ = np.int(w_*10000)\n",
    "        ell_cont[w__] = [e1[mask_],e2[mask_]]\n",
    "\n",
    "    e1_ = np.zeros(len(wi))\n",
    "    e2_ = np.zeros(len(wi))\n",
    "\n",
    "\n",
    "    for w_ in np.unique(wi):\n",
    "        mask_ = (wi*10000).astype(np.int) == np.int(w_*10000)\n",
    "        e1_[mask_] = ell_cont[np.int(w_*10000)][0][np.random.randint(0,len(ell_cont[np.int(w_*10000)][0]),len(e1_[mask_]))]\n",
    "        e2_[mask_] = ell_cont[np.int(w_*10000)][1][np.random.randint(0,len(ell_cont[np.int(w_*10000)][0]),len(e1_[mask_]))]\n",
    "\n",
    "    return e1_,e2_\n",
    "\n",
    "def IndexToDeclRa(index, nside,nest= False):\n",
    "    theta,phi=hp.pixelfunc.pix2ang(nside ,index,nest=nest)\n",
    "    return -np.degrees(theta-np.pi/2.),np.degrees(phi)\n",
    "\n",
    "def convert_to_pix_coord(ra, dec, nside=1024):\n",
    "    \"\"\"\n",
    "    Converts RA,DEC to hpix coordinates\n",
    "    \"\"\"\n",
    "\n",
    "    theta = (90.0 - dec) * np.pi / 180.\n",
    "    phi = ra * np.pi / 180.\n",
    "    pix = hp.ang2pix(nside, theta, phi, nest=False)\n",
    "\n",
    "    return pix\n",
    "\n",
    "def apply_random_rotation(e1_in, e2_in):\n",
    "    np.random.seed() # CRITICAL in multiple processes !\n",
    "    rot_angle = np.random.rand(len(e1_in))*2*np.pi #no need for 2?\n",
    "    cos = np.cos(rot_angle)\n",
    "    sin = np.sin(rot_angle)\n",
    "    e1_out = + e1_in * cos + e2_in * sin\n",
    "    e2_out = - e1_in * sin + e2_in * cos\n",
    "    return e1_out, e2_out\n",
    "\n",
    "def addSourceEllipticity(self,es,es_colnames=(\"e1\",\"e2\"),rs_correction=True,inplace=False):\n",
    "\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\t:param es: array of intrinsic ellipticities, \n",
    "\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\t#Safety check\n",
    "\t\tassert len(self)==len(es)\n",
    "\n",
    "\t\t#Compute complex source ellipticity, shear\n",
    "\t\tes_c = np.array(es[es_colnames[0]]+es[es_colnames[1]]*1j)\n",
    "\t\tg = np.array(self[\"shear1\"] + self[\"shear2\"]*1j)\n",
    "\n",
    "\t\t#Shear the intrinsic ellipticity\n",
    "\t\te = es_c + g\n",
    "\t\tif rs_correction:\n",
    "\t\t\te /= (1 + g.conjugate()*es_c)\n",
    "\n",
    "\t\t#Return\n",
    "\t\tif inplace:\n",
    "\t\t\tself[\"shear1\"] = e.real\n",
    "\t\t\tself[\"shear2\"] = e.imag\n",
    "\t\telse:\n",
    "\t\t\treturn (e.real,e.imag)\n",
    "        \n",
    "        \n",
    "def g2k_sphere(gamma1, gamma2, mask, nside=1024, lmax=2048,nosh=True):\n",
    "    \"\"\"\n",
    "    Convert shear to convergence on a sphere. In put are all healpix maps.\n",
    "    \"\"\"\n",
    "\n",
    "    gamma1_mask = gamma1 * mask\n",
    "    gamma2_mask = gamma2 * mask\n",
    "\n",
    "    KQU_masked_maps = [gamma1_mask, gamma1_mask, gamma2_mask]\n",
    "    alms = hp.map2alm(KQU_masked_maps, lmax=lmax, pol=True)  # Spin transform!\n",
    "\n",
    "\n",
    "    ell, emm = hp.Alm.getlm(lmax=lmax)\n",
    "    if nosh:\n",
    "        almsE = alms[1] * 1. * ((ell * (ell + 1.)) / ((ell + 2.) * (ell - 1))) ** 0.5\n",
    "        almsB = alms[2] * 1. * ((ell * (ell + 1.)) / ((ell + 2.) * (ell - 1))) ** 0.5\n",
    "    else:\n",
    "        almsE = alms[1] * 1.\n",
    "        almsB = alms[2] * 1. \n",
    "    almsE[ell == 0] = 0.0\n",
    "    almsB[ell == 0] = 0.0\n",
    "    almsE[ell == 1] = 0.0\n",
    "    almsB[ell == 1] = 0.0\n",
    "\n",
    "\n",
    "\n",
    "    almssm = [alms[0], almsE, almsB]\n",
    "\n",
    "\n",
    "    kappa_map_alm = hp.alm2map(almssm[0], nside=nside, lmax=lmax, pol=False)\n",
    "    E_map = hp.alm2map(almssm[1], nside=nside, lmax=lmax, pol=False)\n",
    "    B_map = hp.alm2map(almssm[2], nside=nside, lmax=lmax, pol=False)\n",
    "\n",
    "    return E_map, B_map, almsE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042109e2-5cb7-4c91-b783-09ee7cfb982e",
   "metadata": {},
   "source": [
    "# read files & make DES-like shear maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a868731-6374-4516-8e46-107500bae1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/global/cfs/cdirs//desi/public/cosmosim/boryanah_AbacusLensing/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9b2ede-4276-4d14-9670-6f4c9af88323",
   "metadata": {},
   "source": [
    "brief descriptions of the maps (https://abacussummit.readthedocs.io/en/latest/simulations.html)\n",
    "AbacusSummit_base_c000* : 25 realisations at fixed cosmology\n",
    "AbacusSummit_base_c{100-126}_*: linear derivative grid\n",
    "AbacusSummit_base_c{130-181}: emulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d317674b-d8c4-4c14-948f-f02ce4f44615",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = 'AbacusSummit_base_c000_ph000/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff6bd4c-d09b-4d1a-b056-1c53bb9a80fc",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eba8e850-441e-4b31-b7bb-c536a8a4b7a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "config = dict()\n",
    "# DES redhsift distributions\n",
    "config['2PT_FILE'] = '/global/homes/m/mgatti/Mass_Mapping/HOD/PKDGRAV_CODE//2pt_NG_final_2ptunblind_02_26_21_wnz_maglim_covupdate_6000HR.fits'  \n",
    "config['sources_bins'] = [0,1,2,3]\n",
    "#nside of the final maps\n",
    "config['nside'] = 1024\n",
    "\n",
    "\n",
    "# cosmology\n",
    "#https://abacussummit.readthedocs.io/en/latest/cosmologies.html#cosmologies-table\n",
    "config['h'] = 0.6736\n",
    "config['om'] = 0.1200/((config['h']**2))+ 0.02237/((config['h']**2))\n",
    "config['w0'] = -1.\n",
    "\n",
    "z_bin_edges = np.linspace(0.15,2.55,49)\n",
    "z_bounds     = dict()                                                                                         \n",
    "z_bounds['z-high'] = z_bin_edges[1:]\n",
    "z_bounds['z-low'] = z_bin_edges[:-1]\n",
    "\n",
    "\n",
    "    \n",
    "cosmology = wCDM(H0=config['h']*100.*u.km / u.s / u.Mpc,\n",
    "             Om0=config['om'],\n",
    "             Ode0=1-config['om'],\n",
    "             w0=config['w0'] )\n",
    "    \n",
    "    \n",
    "kappa_pref_evaluated = brk.kappa_prefactor(cosmology.H0, cosmology.Om0, length_unit = 'Mpc')\n",
    "comoving_edges =  cosmology.comoving_distance(z_bin_edges)\n",
    "z_centre = np.array([z_at_value(cosmology.comoving_distance, 0.5*(comoving_edges[i]+comoving_edges[i+1]))  for i in range(len(comoving_edges)-1)])\n",
    "comoving_edges =  [cosmology.comoving_distance(x_) for x_ in np.array((z_bin_edges))]\n",
    "un_ = comoving_edges[0].unit\n",
    "comoving_edges = np.array([c.value for c in comoving_edges])\n",
    "comoving_edges = comoving_edges*un_\n",
    "\n",
    "# IA factor\n",
    "c1 = (5e-14 * (u.Mpc**3.)/(u.solMass * u.littleh**2) ) \n",
    "c1_cgs = (c1* ((u.littleh/(cosmology.H0.value/100))**2.)).cgs\n",
    "rho_c1 = (c1_cgs*cosmology.critical_density(0)).value\n",
    "\n",
    "# redshift distributions\n",
    "mu = fits.open(config['2PT_FILE'])\n",
    "redshift_distributions_sources = {'z':None,'bins':dict()}\n",
    "redshift_distributions_sources['z'] = mu[6].data['Z_MID']\n",
    "for ix in config['sources_bins']:\n",
    "    redshift_distributions_sources['bins'][ix] = mu[6].data['BIN{0}'.format(ix+1)]\n",
    "\n",
    "print ('done')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb09de78-1f07-4db0-b097-61f02f3cfd74",
   "metadata": {},
   "source": [
    "# make intermediate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8be91fe9-5708-459f-a7fb-5c986c585422",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = '/pscratch/sd/m/mgatti/Abacus/'\n",
    "if not os.path.exists(outputs+sim):\n",
    "    os.mkdir(outputs+sim)\n",
    "if not os.path.exists(outputs+sim+'/intermediate/'):\n",
    "    os.mkdir(outputs+sim+'/intermediate/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e839904d-e00a-47b5-98a1-3751d839e815",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "for i in range(len(z_bin_edges)-1):\n",
    "    path_ = outputs+sim+'/intermediate/g1g2_{0}.fits'.format(i)\n",
    "    if not os.path.exists(path_):\n",
    "        if i<10:\n",
    "            m = asdf.open(path+sim+'kappa_0000{0}.asdf'.format(i))\n",
    "        else:\n",
    "            m = asdf.open(path+sim+'kappa_000{0}.asdf'.format(i))\n",
    "        kappa_out_ = hp.ud_grade(m['data']['kappa'],nside_out=config['nside'])\n",
    "\n",
    "        del m\n",
    "        gc.collect()\n",
    "        \n",
    "        # make a full sky out of it ----\n",
    "        kappa_out = copy.deepcopy(kappa_out_)\n",
    "        kappa_out += rotate_map_approx(kappa_out,[ 90 ,0 , 0], flip=False,nside = config['nside'])\n",
    "        kappa_out += rotate_map_approx(kappa_out,[ 180 ,0 , 0], flip=False,nside = config['nside'])\n",
    "        kappa_out += rotate_map_approx(kappa_out,[ 180 ,180 , 0], flip=False,nside = config['nside'])\n",
    "\n",
    "        # compute shear e1 and e2\n",
    "        g1, g2 = gk_inv(kappa_out,kappa_out*0,config['nside'],config['nside']*2)\n",
    "\n",
    "\n",
    "\n",
    "        fits_f = Table()\n",
    "        fits_f['g1'] = g1\n",
    "        fits_f['g2'] = g2\n",
    "        #fits_f['g1_IA'] = g1_IA\n",
    "        #fits_f['g2_IA'] = g2_IA\n",
    "        fits_f.write(path_)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e34f81e6-c4ff-4d43-8465-dfdcb301e1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "g1_tomo = dict()\n",
    "g2_tomo = dict()\n",
    "d_tomo = dict()\n",
    "nz_kernel_sample_dict = dict()\n",
    "\n",
    "\n",
    "for tomo_bin in config['sources_bins']:\n",
    "    g1_tomo[tomo_bin] = np.zeros(hp.nside2npix(config['nside']))\n",
    "    g2_tomo[tomo_bin] = np.zeros(hp.nside2npix(config['nside']))\n",
    "    d_tomo[tomo_bin] = np.ones(hp.nside2npix(config['nside']))\n",
    "    redshift_distributions_sources['bins'][tomo_bin][250:] = 0.\n",
    "    nz_sample = brk.recentre_nz(np.array(z_bin_edges).astype('float'),  redshift_distributions_sources['z'],  redshift_distributions_sources['bins'][tomo_bin] )\n",
    "    nz_kernel_sample_dict[tomo_bin] = nz_sample*(z_bin_edges[1:]-z_bin_edges[:-1])\n",
    "\n",
    "    \n",
    "\n",
    "    for i in (range(0,len(comoving_edges)-1)):\n",
    "\n",
    "        #try:\n",
    "           \n",
    "            #path_ = path_folder_output+'/lens_{0}_{1}.fits'.format(shell,config['nside_intermediate'])\n",
    "            #pathk_ = path_folder_output+'/kappa_{0}_{1}.fits'.format(shell,config['nside_intermediate'])\n",
    "            pathgg_ = outputs+sim+'/intermediate/g1g2_{0}.fits'.format(i)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            k_ = fits.open(pathgg_)\n",
    "            #k_real_ = pf.open(pathk_)\n",
    "            #d_ = pf.open(path_)\n",
    "            #IA_f = iaa.F_nla(z_centre[i], cosmology.Om0, rho_c1=rho_c1,A_ia = config['A_IA'], eta=config['eta_IA'], z0=config['z0_IA'],  lbar=0., l0=1e-9, beta=0.)\n",
    "            #print ((k_[1].data['T']))\n",
    "\n",
    "\n",
    "            try:\n",
    "                g1_tomo[tomo_bin]  +=  ((1.+BIAS_SC*(d_[1].data['T']))*(k_[1].data['g1']+k_[1].data['g1_IA']*IA_f))*nz_kernel_sample_dict[tomo_bin][i]\n",
    "                g2_tomo[tomo_bin]  +=  ((1.+BIAS_SC*(d_[1].data['T']))*(k_[1].data['g2']+k_[1].data['g2_IA']*IA_f))*nz_kernel_sample_dict[tomo_bin][i]\n",
    "            except:\n",
    "                g1_tomo[tomo_bin]  +=  ((k_[1].data['g1'])*nz_kernel_sample_dict[tomo_bin][i])\n",
    "                g2_tomo[tomo_bin]  +=  ((k_[1].data['g2'])*nz_kernel_sample_dict[tomo_bin][i])\n",
    "               \n",
    "            # d_tomo[tomo_bin] +=  (1.+BIAS_SC*d_[1].data['T'])*nz_kernel_sample_dict[tomo_bin][i]\n",
    "        #except:\n",
    "        #    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1ee131-1e6c-4d08-82c7-5e53017464b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59189e34-3ee9-4996-8a50-d93dd565fffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_cat = dict()\n",
    "# load the empirical depth - number density relation from the des y3 catalog\n",
    "depth_weigth = np.load('/global/cfs/cdirs/des/mass_maps/Maps_final/depth_maps_Y3_{0}_numbdensity.npy'.format(config['nside']),allow_pickle=True).item()\n",
    "for tomo_bin in config['sources_bins']:\n",
    "    sources_cat[tomo_bin] = dict()\n",
    "    \n",
    "    # load des y3 catalog\n",
    "    mcal_catalog = load_obj('/global/cfs/cdirs/des/mass_maps/Maps_final/data_catalogs_weighted_{0}'.format(tomo_bin))\n",
    "\n",
    "    pix_ = convert_to_pix_coord(mcal_catalog['ra'], mcal_catalog['dec'], nside=config['nside'])\n",
    "    mask = np.in1d(np.arange(hp.nside2npix(config['nside'])),pix_)\n",
    "\n",
    "\n",
    "    # generate ellipticities ***********************************\n",
    "    df2 = pd.DataFrame(data = {'w':mcal_catalog['w'] ,'pix_':pix_},index = pix_)\n",
    "    # draw a new random number of galaxies based on the average depth - number density des y3 relation\n",
    "    nn = np.random.poisson(depth_weigth[tomo_bin])\n",
    "    \n",
    "    # the following bit draws galaxy ellipticities & weights from the empirical des y3 pixel - weight - ellipticity relation\n",
    "    nn[~mask]= 0\n",
    "    count = 0\n",
    "    nnmaxx = max(nn)\n",
    "    for count in range(nnmaxx):\n",
    "        if count %2 ==0:\n",
    "            df3 = df2.sample(frac=1)\n",
    "            df4 = df3.drop_duplicates('pix_',keep ='first').sort_index()\n",
    "        else:\n",
    "            df4 = df3.drop_duplicates('pix_',keep ='last').sort_index()\n",
    "\n",
    "        pix_valid = np.arange(len(nn))[nn>0]\n",
    "        df3 = df4.loc[np.unique(pix_valid)]\n",
    "        if count == 0:\n",
    "            w = df3['w']\n",
    "            pix = df3['pix_']\n",
    "        else:\n",
    "            w = np.hstack([w,df3['w']])\n",
    "            pix = np.hstack([pix,df3['pix_']]) \n",
    "        nn -= 1\n",
    "\n",
    "    del df2\n",
    "    del df3\n",
    "    gc.collect()\n",
    "    e1,e2 = random_draw_ell_from_w(w,mcal_catalog['w'],mcal_catalog['e1'],mcal_catalog['e2'])\n",
    "\n",
    "\n",
    "\n",
    "    del mcal_catalog\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    f = 1./np.sqrt(d_tomo[tomo_bin]/np.sum(nz_kernel_sample_dict[tomo_bin]))\n",
    "    f = f[pix]\n",
    "\n",
    "\n",
    "    # ++++++++++++++++++++++\n",
    "\n",
    "    n_map_sc = np.zeros(hp.nside2npix(config['nside']))\n",
    "\n",
    "    unique_pix, idx, idx_rep = np.unique(pix, return_index=True, return_inverse=True)\n",
    "\n",
    "\n",
    "    n_map_sc[unique_pix] += np.bincount(idx_rep, weights=w/f**2)\n",
    "\n",
    "    g1_ = g1_tomo[tomo_bin][pix]\n",
    "    g2_ = g2_tomo[tomo_bin][pix]\n",
    "\n",
    "\n",
    "    es1,es2 = apply_random_rotation(e1/f, e2/f)\n",
    "    es1a,es2a = apply_random_rotation(e1/f, e2/f)\n",
    "\n",
    "\n",
    "    x1_sc,x2_sc = addSourceEllipticity({'shear1':g1_,'shear2':g2_},{'e1':es1,'e2':es2},es_colnames=(\"e1\",\"e2\"))\n",
    "\n",
    "\n",
    "    e1r_map = np.zeros(hp.nside2npix(config['nside']))\n",
    "    e2r_map = np.zeros(hp.nside2npix(config['nside']))\n",
    "\n",
    "    e1r_map0 = np.zeros(hp.nside2npix(config['nside']))\n",
    "    e2r_map0 = np.zeros(hp.nside2npix(config['nside']))\n",
    "\n",
    "    g1_map = np.zeros(hp.nside2npix(config['nside']))\n",
    "    g2_map = np.zeros(hp.nside2npix(config['nside']))\n",
    "\n",
    "    unique_pix, idx, idx_rep = np.unique(pix, return_index=True, return_inverse=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    e1r_map[unique_pix] += np.bincount(idx_rep, weights=es1*w)\n",
    "    e2r_map[unique_pix] += np.bincount(idx_rep, weights=es2*w)\n",
    "\n",
    "    e1r_map0[unique_pix] += np.bincount(idx_rep, weights=es1a*w)\n",
    "    e2r_map0[unique_pix] += np.bincount(idx_rep, weights=es2a*w)\n",
    "\n",
    "\n",
    "    g1_map[unique_pix] += np.bincount(idx_rep, weights= g1_*w)\n",
    "    g2_map[unique_pix] += np.bincount(idx_rep, weights= g2_*w)\n",
    "\n",
    "\n",
    "    mask_sims = n_map_sc != 0.\n",
    "    e1r_map[mask_sims]  = e1r_map[mask_sims]/(n_map_sc[mask_sims])\n",
    "    e2r_map[mask_sims] =  e2r_map[mask_sims]/(n_map_sc[mask_sims])\n",
    "    e1r_map0[mask_sims]  = e1r_map0[mask_sims]/(n_map_sc[mask_sims])\n",
    "    e2r_map0[mask_sims] =  e2r_map0[mask_sims]/(n_map_sc[mask_sims])\n",
    "    g1_map[mask_sims]  = g1_map[mask_sims]/(n_map_sc[mask_sims])\n",
    "    g2_map[mask_sims] =  g2_map[mask_sims]/(n_map_sc[mask_sims])\n",
    "\n",
    "    #EE,BB,_   =  g2k_sphere((g1_map+e1r_map0)*nuis['m'][tomo_bin-1], (g2_map+e2r_map0)*nuis['m'][tomo_bin-1], mask_sims, nside=config['nside2'], lmax=config['nside2']*2 ,nosh=True)\n",
    "   # EEn,BBn,_ =  g2k_sphere(e1r_map*nuis['m'][tomo_bin-1], e2r_map*nuis['m'][tomo_bin-1], mask_sims, nside=config['nside2'], lmax=config['nside2']*2 ,nosh=True)\n",
    "    #sources_cat[rot][tomo_bin] = {'kE':EE,'kE_noise':EEn,'mask':mask_sims}\n",
    "\n",
    "\n",
    "    e1_ = ((g1_map+e1r_map0))[mask_sims]\n",
    "    e2_ = ((g2_map+e2r_map0))[mask_sims]\n",
    "    e1n_ = ( e1r_map)[mask_sims]\n",
    "    e2n_ = ( e2r_map)[mask_sims]\n",
    "    idx_ = np.arange(len(mask_sims))[mask_sims]\n",
    "\n",
    "    kE,kB,_ = g2k_sphere(((g1_map+e1r_map0)),  ((g2_map+e2r_map0)), mask, nside=config['nside'], lmax=config['nside']*2,nosh=True)\n",
    "    kEN,kBN,_ = g2k_sphere(e1r_map, e2r_map, mask, nside=config['nside'], lmax=config['nside']*2,nosh=True)\n",
    "    \n",
    "    sources_cat[tomo_bin] = {'e1':e1_,'e2':e2_,'e1n':e1n_,'e2n':e2n_,'pix':idx_,'kE':kE[mask_sims],'kEN':kEN[mask_sims]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200d351d-5fac-4c28-803d-6aa0fc65a708",
   "metadata": {},
   "source": [
    "# jackknife"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cc92a2-af48-43d1-9533-de4cc999a69c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bfd_env3",
   "language": "python",
   "name": "bfd_env3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
